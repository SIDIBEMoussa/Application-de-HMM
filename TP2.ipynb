{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "TP2.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SIDIBEMoussa/Application-de-HMM/blob/main/TP2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeVuh-b0mdqU"
      },
      "source": [
        "# TP2\n",
        "\n",
        "# Introduction\n",
        "\n",
        "- Le but de ce TP est de **se familiariser avec les concepts** des **`modèles de Markov cachés`** (**= HMM**).\n",
        "- La portée de l'ambition est donc limitée et les exemples sont très simples.\n",
        "\n",
        "Sujets principaux:\n",
        "- Formulation du problème à l'aide d'un exemple de base pour introduire la **terminologie HMM**.\n",
        "- Illustration de la **règle de Bayes** pour déterminer l'état caché le plus probable compte tenu d'une observation.\n",
        "- Implémentation du **Forward Algorithm** et du **Backward Algorithm** pour calculer la probabilité d'une séquence d'observation particulière.\n",
        "- Définition et illustration des bénéfices de la **Programmation Dynamique**.\n",
        "- Implémentation de l'**Algorithme de Décodage de Viterbi** pour trouver la séquence d'états cachés la plus probable étant donné une séquence d'observation.\n",
        "- Implémentation de l'**algorithme de Baum-Welch** pour trouver les paramètres HMM les plus probables compte tenu de certaines séquences d'observation.\n",
        "\n",
        "\n",
        "# Motivation du problème\n",
        "\n",
        "- Votre voiture roule sur une **autoroute à 2 voies**.\n",
        "- Imaginez que vous puissiez **suivre à distance la vitesse de la voiture** (par exemple, elle est communiqué).\n",
        "- Mais vous n'avez **pas d'accès direct à la position latérale** ('voie de droite' de 'voie de gauche').\n",
        "- Formellement, vous **ne pouvez pas observer directement la marche stochastique sous-jacente entre les états de « voie »**.\n",
        "- Comment pouvez-vous **déduire la \"voie\"** en vous basant sur les informations uniques que vous recevez (la \"vitesse\") ?\n",
        "### Probabilité d'émission\n",
        "\n",
        "Si je vous dis que je conduis à « basse vitesse », vous **pouvez deviner** que je suis sur la voie de droite.\n",
        "- Par exemple, parce que je conduis seul à un rythme raisonnable.\n",
        "- Soit parce que je suis bloqué par un véhicule lent alors que je n'arrive pas à le reprendre.\n",
        "- Mais je pourrais aussi rouler vite sur cette « voie de droite »\n",
        "\n",
        "De même, si vous êtes informé d'une « vitesse élevée », vous pourriez dire que je suis **plus susceptible** de conduire sur la voie de gauche.\n",
        "- Probablement en dépassant un autre véhicule.\n",
        "- Néanmoins, ce n'est **pas toujours vrai** : Pensez à la situation où vous attendez sur la voie de gauche derrière un camion essayant de dépasser un autre camion.\n",
        "\n",
        "On obtient ici une **première intuition** :\n",
        "- La variable \"voie\" semble avoir un impact sur la variable \"vitesse\".\n",
        "- En d'autres termes : **vous ne roulez pas au même rythme selon que vous êtes sur la « voie de gauche » ou la « voie de droite »**.\n",
        "- Mais la relation n'est **pas déterministe**. C'est **stochastique**.\n",
        "\n",
        "Ce **résultat de causalité** sera modélisé à l'aide de **`probabilités d'émission`** dans ce qui suit.\n",
        "\n",
        "### Probabilité de transition\n",
        "\n",
        "Vous pourriez avoir une deuxième intuition sur le **processus séquentiel** :\n",
        "- Les conducteurs humains **restent généralement sur leurs voies**.\n",
        "- Par conséquent, si vous êtes sur la « voie de droite » au temps « t », vous êtes probablement toujours sur la « voie de droite » au temps « t+1 ».\n",
        "- Encore une fois, cela **ne tient pas toujours** et vous pouvez trouver des **exceptions**.\n",
        "- Mais voici une seconde intuition : **la `voie` à l'instant `t` est influencée par la `voie` à l'instant `t-1`**.\n",
        "\n",
        "Le concept de **`probabilité de transition`** sera utilisé pour modéliser cette seconde remarque.\n",
        "\n",
        "\n",
        "## Terminologie\n",
        "\n",
        "Dans les sections suivantes, nous verrons comment pouvons-nous **soutenir mathématiquement ces intuitions**.\n",
        "\n",
        "| ![La `vitesse` est `l'observation` tandis que la `voie` constitue `l'état caché`. Quelques exemples montrent que toutes les `émissions` sont possibles.](docs/terminology.PNG \"La `vitesse` est l `observation` tandis que la `voie` constitue `l'état caché`. Certains exemples montrent que toutes les `émissions` sont possibles .\") |\n",
        "|:--:|\n",
        "| *La `vitesse` est `l'observation` tandis que la `voie` constitue `l'état caché`. Quelques exemples montrent que toutes les `émissions` sont possibles.* |\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Objectifs\n",
        "\n",
        "Nous pouvons maintenant définir trois problèmes qui peuvent être résolus par un HMM :\n",
        "\n",
        "- 1- **Apprentissage**\n",
        "\t- Le premier est **l'apprentissage des paramètres** de sa **structure latente** (modèle 'émission', modèle 'transition' et distribution 'état initial').\n",
        "\t- Dans le cas d'une structure connue et d'un **échantillonnage pleinement observable**, on peut appliquer le concept d'**Estimation de Vraisemblance Maximale** (MLE) :\n",
        "\t\t- Certaines séquences d'observation (`vitesse`) et leurs états associés (`voie`) ont été collectés. Les **échantillons** forment le **ensemble de données d'entraînement**.\n",
        "\t\t-  Les paramètres peuvent être sélectionnés de manière à **maximiser la probabilité** pour le modèle d'**avoir produit les données** à partir de l'ensemble de données donné.\n",
        "\t\t-  La question [Q1](#q1) introduit cette méthode d'**apprentissage supervisé**.\n",
        "\t- S'il n'est **pas possible d'échantillonner à partir d'états cachés**, optez pour l'**apprentissage non supervisé**.\n",
        "\t\t- Une méthode basée sur la **Maximisation des attentes** (EM) est présentée dans [Q6](#q6).\n",
        "\n",
        "- 2- **Évaluation**\n",
        "\t- Une fois la structure du HMM définie et ses paramètres déterminés, la deuxième tâche consiste à trouver ** quelle est la probabilité qu'il obtienne une séquence d'observation particulière **.\n",
        "\t- Ce problème, parfois appelé **\"Scoring\"**, est traité à la question [Q3](#q3).\n",
        "\n",
        "- 3- **Inférence**\n",
        "\t- Dans le troisième problème, nous voulons **déduire la séquence de voies** parcourues par la voiture ((`droit` ou `gauche`) = **état caché**) sur la base d'une **séquence de mesures de vitesse* * (= **observations**).\n",
        "\t- Quatre types d'inférence peuvent être distingués :\n",
        "\t\t- **Filtrage** : déterminer le **dernier état de croyance**, c'est-à-dire la distribution postérieure p(`voie(t)` | [`vitesse(1)`, ..., `vitesse(t)`]) . Ceci est détaillé dans [Q4](#q4).\n",
        "\t\t- **Décodage** : déterminer l'état caché **complet** **séquence** qui donne la **meilleure explication** pour l'émission de la séquence d'observation, comme expliqué dans [Q5](#q5).\n",
        "\t\t- **Prédiction** : déterminer la probabilité de l'**état caché futur en `k` pas**, c'est-à-dire la distribution conditionnelle postérieure p(`voie(t+k)` | [`vitesse(1)`, . .., `vitesse(t)`] ). Il est mentionné dans [Q4](#q4).\n",
        "\t\t- **Lissage** : déterminez la probabilité pour l'**état caché passé il y a `k` pas**, c'est-à-dire la distribution conditionnelle postérieure p(`voie(t-k)` | [`vitesse(1)`, ... , `vitesse(t)`] ). Il est mentionné dans [Q4](#q4).\n",
        "\t\t- Avant d'appliquer ces techniques aux séquences, [Q2](#q2) montre comment faire des inférences pour des **observations simples**.\n",
        "\n",
        "## Hypothèses\n",
        "Pour garder le problème aussi simple que possible :\n",
        "- **Discrétisons** la `vitesse` **variable d'observation** en `faible vitesse` et `haute vitesse`.\n",
        "- Les pas de temps sont également discrétisés.\n",
        "- Les transitions de voie sont ignorées : soit vous êtes sur la « voie de gauche », soit vous êtes sur la « voie de droite ».\n",
        "\n",
        "### Processus stationnaire\n",
        "\n",
        "- On suppose que les modèles HMM (matrice de transition, matrice d'émission) restent **constants dans le temps**.\n",
        "- p[`vitesse(t)` | `voie(t)`] et p[`voie(t+1)` | `voie(t)`] sont indépendants de `t`.\n",
        "\n",
        "### Observation Indépendance\n",
        "\n",
        "- Nous avons parlé de probabilité d'émission, expliquant que l'état `voie(t)` impacte l'observation `vitesse(t)` émise au même pas de temps (`t`).\n",
        "\t- On pourrait imaginer d'autres sources d'influence : « vitesse(t-1) » et « voie(t-1) » par exemple.\n",
        "- Ici, nous supposons que la **probabilité d'une observation ne dépend que de l'état qui a produit l'observation** et non de tout autre état ou de toute autre observation.\n",
        "\t- En d'autres termes, chaque variable d'observation \"vitesse\" ne dépend que de l'état courant \"voie\".\n",
        "\t- Il s'agit d'une **hypothèse forte** puisque nous décidons de ne pas capturer les dépendances directes entre chaque élément de la séquence d'observation.\n",
        "\t- Mais cela va massivement **relâcher le calcul** pendant l'inférence.\n",
        "- La **probabilité conditionnelle conjointe** suivante peut être simplifiée :\n",
        "\t- p(`vitesse(t)` | `voie(1)` ... `voie(t)`, `vitesse(1)` ... `vitesse(t-1)`) = p(`vitesse( t)` | `voie(t)`).\n",
        "\n",
        "### Propriété de Markov de premier ordre\n",
        "\n",
        "- Nous venons de dire qu'il est utile de connaître la `voie` présente (au temps `t`) pour en déduire la future `voie` (au temps `t+1`).\n",
        "\n",
        "- Voici une hypothèse forte sur l'inférence dans ce processus stochastique :\n",
        "\t- La distribution de probabilité conditionnelle des **états futurs** (conditionnellement aux états passés et présents) **dépend uniquement de l'état présent**, pas de la séquence d'événements qui l'ont précédé.\n",
        "- En d'autres termes, **\"le futur est indépendant du passé étant donné le présent\"**.\n",
        "- Cette **hypothèse forte** est connue sous le nom de **Propriété de Markov** de premier ordre (également appelée **\"propriété sans mémoire\"**) et facilitera nos calculs.\n",
        "\n",
        "Sur la base de ces hypothèses, le problème peut être modélisé à l'aide d'un **modèle graphique** :\n",
        "- Les HMM sont des **modèles orientés** (d'où des flèches) puisqu'on peut distinguer quelle est la raison (état de la voie) et quel est le résultat (observation de la vitesse).\n",
        "- La **Propriété de Markov** implique des connexions entre états consécutifs.\n",
        "- L'**indépendance de sortie** fait que chaque observation ne reçoit qu'un seul front (provenant de l'état associé).\n",
        "\n",
        "| ![Représentation graphique HMM.](docs/hmm_graphical_model.PNG \"Représentation graphique HMM.\")  | \n",
        "|:--:| \n",
        "| *Représentation graphique HMM.* |\n",
        "\n",
        "# Formulation du problème\n",
        "\n",
        "## Définition :\n",
        "\n",
        "Un modèle discret de Markov caché (HMM) est un **5-tuple** composé de :\n",
        "\n",
        "- Un ensemble d'**États cachés** : variable aléatoire discrète `lane` dans {`right_lane`, `left_lane`}.\n",
        "- Un ensemble d'**Observations** possibles : variable aléatoire discrète `speed` dans {`low_speed`, `high_speed`}.\n",
        "- Une matrice stochastique qui donne des **probabilités d'émission** : p[`speed(t)` | `lane(t)`].\n",
        "- Une matrice stochastique qui donne des **probabilités de transition** : p[`lane(t+1)` | `lane(t)`].\n",
        "- Une distribution **Probabilité d'état initial** : p[`lane(t=1)`].\n",
        "\n",
        "# Les questions\n",
        "\n",
        "- [Q1](#q1) - Comment **estimer les paramètres** de notre HMM ?\n",
        "- [Q2](#q2) - Étant donné une **observation unique de « vitesse »**, quelle est la probabilité que la voiture se trouve dans chacune des deux voies ?\n",
        "- [Q3](#q3) - Quelle est la probabilité d'observer une **séquence particulière de mesures de « vitesse »** ?\n",
        "- [Q4](#q4) - Étant donné une **séquence d'observations de « vitesse »**, quelle est la **« voie »** actuelle la plus probable ?\n",
        "- [Q5](#q5) - Étant donné une **séquence d'observations de « vitesse »**, quelle est la **séquence de « voies » sous-jacente la plus probable** ?\n",
        "- [Q6](#q6) - Comment **estimer les paramètres** de notre HMM lorsque **aucune annotation \"d'état\"** n'est présente dans les données d'entraînement ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-iQyzPBot3X"
      },
      "source": [
        "# Réponses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMw1pe_boyHG"
      },
      "source": [
        "\n",
        "\n",
        "*   [Q1](q1)- On estime par la méthode brownien\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MeEuPcymhFQ",
        "outputId": "02c8aac8-05c7-4c58-89ce-74016f99a82a"
      },
      "source": [
        "from tp1_functions import *"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Interations:  9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBp4COfNos2q"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KGVYZu_oHFg"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\"\"\"\n",
        "Compte tenu des observations\n",
        "\"\"\"\n",
        "O = np.array([[0,1,0,1]]) #observation en séquence\n",
        "T = len(O[0]) #nombre d'observations en séquence\n",
        "    \n",
        "\"\"\"\n",
        "Initialiser les matrices\n",
        "\"\"\"\n",
        "A = np.array([[12/15,3/15],\n",
        "              [4/10,6/10]]) \n",
        "B = np.array([[0.2,0.8],\n",
        "              [0.6,0.4]]) \n",
        "pi = np.array([15/25,10/25]) \n",
        "\n",
        "\"\"\"\n",
        "Limites globales\n",
        "\"\"\"\n",
        "N = np.shape(B)[0]\n",
        "M = np.shape(B)[1]"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "id": "zD-cTslZrnoz",
        "outputId": "eb153064-87b9-4d71-bb05-a404bdee0493"
      },
      "source": [
        "A,B,pi,alpha,beta,gamma,digam=markov(O,N)\n",
        "pi"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-66948fb849ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdigam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmarkov\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/tp1_functions.py\u001b[0m in \u001b[0;36mmarkov\u001b[0;34m(O, N)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0miters\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmaxIters\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlogProb\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0moldLogProb\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.000001\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbpass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdigamma\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdigam\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/tp1_functions.py\u001b[0m in \u001b[0;36mapass\u001b[0;34m(A, B, pi, alpha, N, T, c)\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                 \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 1 with size 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eik2n36YrrqJ",
        "outputId": "4894fda7-f782-41bb-bddd-1c20954b2c10"
      },
      "source": [
        "print(A)\n",
        "print(\"=\"*20)\n",
        "print(B)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[2.07652184e-035 1.00000000e+000]\n",
            " [1.00000000e+000 1.94052162e-173]]\n",
            "====================\n",
            "[[1.00000000e+000 2.16632144e-070 2.07652184e-035]\n",
            " [1.28676510e-139 5.00000000e-001 5.00000000e-001]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7WQ3GTMrwLW",
        "outputId": "69729b59-c47b-4016-a2ca-72bdd031821c"
      },
      "source": [
        "p_state(gamma)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTniMD8ms3DQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}